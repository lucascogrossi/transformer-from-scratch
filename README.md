# Transformer from Scratch

An encoder-decoder transformer model implemented from scratch, based on the paper **["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762)**. The goal is to understand the core components of the Transformer architecture by building it step by step.

The model was trained on parallel English-Portuguese text to perform machine translation using the [OPUS Books dataset](https://huggingface.co/datasets/Helsinki-NLP/opus_books).



![transformer](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)